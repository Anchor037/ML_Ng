### 机器学习-吴恩达网易云课堂笔记

1. **Machine Learning**: A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measure by P, improves with experience E.

2. **Supervised learning, Unsupervised learning; Regression, Classification.**

3. **Linear regression**
   - Hypothesis function : $h_\theta(x) = \theta_0 + \theta_1x$
   - Cost function : $J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2$   (Squared error function)
   - Goal : $\underset{\theta_0, \theta_1}{minimize} J(\theta_0, \theta_1)$

4. **Gradient descent**

   - 用于最小化代价函数。这里以上述线性回归为例子描述该方法具体流程。

   - 算法过程 
     $$
     repeat\ until\ convergence \  \{            \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \ \\ 
         \theta_j := \theta_j - \alpha\frac{\partial }{\partial \theta_j}J(\theta_0, \theta_1) \quad (for \ j = 0 \ and \  j = 1)            \quad \quad \quad 　 \\
     \}   \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad   \quad\quad   \quad \quad \quad \quad \quad \quad \quad \quad \quad
     $$
     其中 $\alpha$称为学习率，$\alpha$越大代表梯度下降的越快，$\alpha$越小代表梯度下降的越慢。

     如果 $\alpha$太小的话梯度下降的会比较慢， $\alpha$太大的话梯度下降可能会越过最低点甚至无法收敛或者发散。

     注：$\theta_0$和$\theta_1$需要同时更新，更新方式如下：
     $$
     temp0 := \theta_0 - \alpha\frac{\partial }{\partial \theta_0}J(\theta_0, \theta_1)\\
     temp1 := \theta_1 - \alpha\frac{\partial }{\partial \theta_1}J(\theta_0, \theta_1)\\
     \theta_0 := temp0\\
     \theta_1 := temp1
     $$

   - Gradient descent for linear regression
     $$
     j = 0 :\ \frac{\partial }{\partial \theta_0}J(\theta_0, \theta_1) = \frac{1}{m}\sum_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)}) \quad\quad \\\
     j = 1 :\ \frac{\partial }{\partial \theta_1}J(\theta_0, \theta_1) = \frac{1}{m}\sum_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)}) x^{(i)} \ \ \
     $$

   - 这种梯度下降算法也称为**“Batch” Gradient descent**, "Batch" : Each step of gradient descent uses all the training examples.

   - 梯度下降法的**缺点**：需要选择$\alpha$；需要多次迭代。

5. **Linear regression with multiple variables**

   - Hypothesis function : $h_\theta(x) = \theta^Tx = \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + \cdots + \theta_nx_n$

   - Cost function : $J(\theta_0,\theta_1,\cdots ,\theta_n) = \frac{1}{2m}\sum_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2$ 

   - Goal : $\underset{\theta_0, \theta_1, \cdots ,\theta_n}{minimize} J(\theta_0, \theta_1, \cdots ,\theta_n)$

   - Gradient descent
     $$
     repeat\  \{         \quad    \quad  \quad  \quad  \quad  \quad  \quad  \quad  \quad  \quad  \quad    \quad \quad \quad \quad \quad \quad \quad \quad  \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad  \ \\ 
         \theta_j := \theta_j - \alpha\frac{\partial }{\partial \theta_j}J(\theta_0, \theta_1, \dots, \theta_n) \quad (simultaneously \ update \ for \ every \ j = 0, \dots, n)               　 \\
     \}   \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad   \quad\quad   \quad \quad \quad \quad \quad \quad \quad \quad \quad  \quad  \quad  \quad  \quad  \quad  \quad  \quad  \quad  \quad    \quad \\
     其中\frac{\partial }{\partial \theta_ｊ}J(\theta_0, \theta_1, \dots, \theta_n) = \frac{1}{m}\sum_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \quad \quad \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad
     $$

   - **特征缩放(feature scaling)**，当不同的特征取值范围相差很大时，可以进行特征缩放，使所有的特征取值范围大致相同，这样可以加快梯度下降的速度。常用**均值归一化(mean normalization)**来进行特征缩放。

   - **正规方程法(normal equation)**，求解可得$\theta = (X^TX)^{-1}X^Ty$。注：由于需要计算$(X^TX)^{-1}$，故**当特征数很多的时候运行的非常慢**。

6. **Logistic regression**

   - 逻辑回归是一种**分类**算法。

   - Hypothesis  : $h_\theta(x) = g(\theta^Tx), \quad g(z) = \frac{1}{1 + e^{-z}}$.

   - predict "$y = 1$" if $h_\theta(x) \geqslant 0.5$ ; predict "$y = 0$" if $h_\theta(x) < 0.5$.

   - Cost function
     $$
     J(\theta) = \frac{1}{m}\sum_{i = 1}^{m}Cost(h_\theta(x^{(i)}), y^{(i)}) \\
     Cost(h_\theta(x), y) = \left\{
     \begin{aligned}
     -log(h_\theta(x)) \quad if \ y = 1\\
     -log(1 - h_\theta(x)) \quad if \ y = 0\\
     \end{aligned}
     \right.
     $$
     注：这里代价函数不用与线性回归一样的平方误差函数是因为$g(z) = \frac{1}{1 + e^{-z}}$是非线性函数，如果还用平方误差函数作为代价函数的话，会导致代价函数非凸，这样再用梯度下降法求解的话很容易陷入局部最优解。

     化简上述Cost等式，可得$Cost(h_\theta(x), y) = -ylog(h_\theta(x)) - (1 - y)log(1 - h_\theta(x))$，那么$J(\theta) = -\frac{1}{m}[\sum_{i = 1}^{m}(y^{(i)}log(h_\theta(x^{(i)})) + (1 - y^{(i)})log(1 - h_\theta(x^{(i)}))]$

   - Gradient descent

     want $min_\theta J(\theta)$ :
     $$
     repeat\  \{      \quad  \quad  \quad  \quad  \quad  \quad  \quad  \quad  \quad    \quad \quad \quad \quad \quad \quad \quad \quad  \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad  \ \\ 
         \theta_j := \theta_j - \alpha\frac{\partial }{\partial \theta_j}J(\theta) \quad (simultaneously \ update \ all \ \theta_j)         \quad\quad\quad\quad\quad\quad\quad\quad      　 \\
     \}   \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad   \quad\quad   \quad \quad \quad \quad \quad \quad \quad \quad \quad  \quad  \quad  \quad  \quad  \quad  \quad  \quad  \quad  \quad    \quad \\
     其中\frac{\partial }{\partial \theta_ｊ}J(\theta) = \frac{1}{m}\sum_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \quad \quad \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad
     $$

7. **Regularization**

   - 用于减少**过拟合(overfitting)**问题。
   - 解决过拟合方法：减少特征数量；正则化。
   - Regularized linear regression : $J(\theta) = \frac{1}{2m}[\sum_{i = 1}^m(h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda \sum_{j = 1}^n\theta_j^2]$
   - Regularized logistic regression : $J(\theta) = -\frac{1}{m}[\sum_{i = 1}^{m}(y^{(i)}log(h_\theta(x^{(i)})) + (1 - y^{(i)})log(1 - h_\theta(x^{(i)})))] + \frac{\lambda}{2m}\sum_{j = 1}^n\theta_j^2$

8. **Neural Network**

   - 神经元：$h_\theta(x) = g(\theta^Tx), \quad g(z) = \frac{1}{1 + e^{-z}}$

   - 神经网络的基本模型

     <img src="http://m.qpic.cn/psb?/V10Imkq237hnz8/fbeOYRE5M6DieHVXEM5Nq24UJzxPfzOX0x7F2Qo51JI!/b/dDABAAAAAAAA&bo=2gJ6AQAAAAADF5E!&rf=viewer_4" style="width:90px height:140px" /><img src="http://m.qpic.cn/psb?/V10Imkq237hnz8/345kdgU*gmoyhJkiN9CpxviolxUz98ZYCtajqcKsZLM!/b/dPMAAAAAAAAA&bo=uQIZAQAAAAADF5E!&rf=viewer_4" style="width:90px height:135px" />

     前馈传播 
     $$
     a^{(2)}_1 = g(z_1^{(2)}), z_1^{(2)} = \Theta^{(1)}_{10}x_0 + \Theta^{(1)}_{11}x_1 + \Theta^{(1)}_{12}x_2 + \Theta^{(1)}_{13}x_3\\ 
     a^{(2)}_2 = g(z_2^{(2)}), z_2^{(2)} = \Theta^{(1)}_{20}x_0 + \Theta^{(1)}_{21}x_1 + \Theta^{(1)}_{22}x_2 + \Theta^{(1)}_{23}x_3\\
     a^{(2)}_3 = g(z_3^{(2)}), z_3^{(2)} = \Theta^{(1)}_{30}x_0 + \Theta^{(1)}_{31}x_1 + \Theta^{(1)}_{32}x_2 + \Theta^{(1)}_{33}x_3 \\
     h_\Theta(x) = a^{(3)}_1 = g(\Theta^{(2)}_{10}a^{(2)}_0 +\Theta^{(2)}_{11}a^{(2)}_1 + \Theta^{(2)}_{12}a^{(2)}_2 + \Theta^{(2)}_{13}a^{(2)}_3) \\
     由x = \begin{bmatrix}
     x_0
     \\ x_1
     \\ x_2
     \\ x_3
     
     \end{bmatrix}，z^{(2)} = \begin{bmatrix}
     z_1^{(2)}
     \\ z_2^{(2)}
     \\ z_3^{(2)}
     
     \end{bmatrix}　\quad 可得z^{(2)} = \Theta^{(1)}x, a^{(2)} = g(z^{(2)}) \quad 【向量化】
     $$

   - 为了更全面的理解神经网络模型，下面用一个稍微复杂一点的模型来举例。

     <img src="http://m.qpic.cn/psb?/V10Imkq237hnz8/gCeOIwz*gfpD2smHMA*f07*6vsthx3ySHXHgIKMKneo!/b/dLgAAAAAAAAA&bo=KwIkAQAAAAADBy4!&rf=viewer_4" />

     前馈传播【向量化】
     $$
     a^{(1)} = x \quad\quad\quad\quad\quad\quad\quad\ \ \\
     z^{(2)} = \Theta^{(1)}a^{(1)} \quad\quad\quad\quad\ \  \\
     a^{(2)} = g(z^{(2)}) \quad (add \ a_0^{(2)}) \\
     z^{(3)} = \Theta^{(2)}a^{(2)} \quad\quad\quad\quad\ \  \\
     a^{(3)} = g(z^{(3)}) \quad (add \ a_0^{(3)}) \\
     z^{(4)} = \Theta^{(3)}a^{(3)} \quad\quad\quad\quad\ \ \\
     a^{(4)} = h_\Theta(x) = g(z^{(4)}) \quad \
     $$

   - Cost funcstion

     假设进行多元分类，类别数为$K$。
     $$
     h_\Theta(x) \in \mathbb{R}^K \quad (h_\Theta(x))_i = i^{th} \ output     \\
     J(\Theta) = -\frac{1}{m}[\sum_{i = 1}^{m}\sum_{k = 1}^{K}(y_k^{(i)}log(h_\Theta(x^{(i)}))_k + (1 - y_k^{(i)})log(1 - (h_\Theta(x^{(i)}))_k))] \\ 
     + \frac{\lambda}{2m}\sum_{l = 1}^{L - 1}\sum_{i = 1}^{s_l}\sum_{j = 1}^{s_{l + 1}}(\Theta_{ji}^{(l)})^2 \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad
     $$
     其中 $L$ = total no. of layers in network, $s_l$ = no. of units (not counting bias unit) in layer $l$ 

   - **反向传播(BP)算法**

     注：主要来求解偏导数。

     预处理一些数值

     Intuition : $\delta_j^{(l)}$ = "error" of node $j$ in layer $l$.

     For each output unit (layer L = 4)

     $\delta^{(4)} = a^{(4)} - y$

     $\delta^{(3)} = (\Theta^{(3)})^T\delta^{(4)}.*g'(z^{(3)})​$

     $\delta^{(2)} = (\Theta^{(2)})^T\delta^{(3)}.*g'(z^{(2)})$

     【可以证明$\frac{\partial}{\partial \Theta_{ij}^{(l)}}J(\Theta) = a_j^{(l)}\delta_i^{(l + 1)}$, 忽略正则化的情况下】

     下面是反向传播算法(**Backpropagation algorithm**)的具体步骤：

     Training set {$(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), \dots, (x^{(m)}, y^{(m)}) $}

     Set $\Delta_{ij}^{(l)} = 0$     (for all $l, i, j$)

     For  $i = 1$ to $m$

     ​	Set $a^{(1)} = x$

     ​	Peform forward propagation to compute $a^{(l)}$ for $l = 2, 3, \dots, L$

     ​	Using $y^{(i)}$, compute $\delta^{(L)} = a^{(L)} - y^{(i)}$

     ​	Compute $\delta^{(L - 1)}, \delta^{L - 2}, \dots, \delta^2$ 

     ​        $\Delta_{ij}^{(l)} := \Delta_{ij}^{(l)} + a_j^{(l)}\delta_i^{(l + 1)}$   【向量化表示：$\Delta^{(l)} := \Delta^{(l)} +\delta^{(l + 1)}(a^{(l)})^T$】

     $D_{ij}^{(l)} := \frac{1}{m}\Delta_{ij}^{(l)} + \lambda\Theta_{ij}^{(l)} \quad if \ j \ne 0$

     $D_{ij}^{(l)} := \frac{1}{m}\Delta_{ij}^{(l)} \quad\quad \quad \ \  \quad if \ j = 0$

     最后可得$\frac{\partial}{\partial \Theta_{ij}^{(l)}}J(\Theta) = D_{ij}^{(l)}$

     求得偏导之后可继续使用梯度下降法或其他高级优化算法。

   - 注：用神经网络进行多元分类时，将最后的输出结果表示成向量的形式。有几种类别，最后一层输出层就有几个神经元，输出结果就是几维向量。此处用到了多元分类的**One vs All**思想。

   - **梯度检验(Gradient checking)**, 利用$\frac{d}{d\theta}J(\theta) \approx \frac{J(\theta + \epsilon) - J(\theta - \epsilon)}{2\epsilon}$来近视计算导数，验证所求得的导数是否正确，进而验证前馈传播算法和反向传播算法计算是否正确。

   - 最后，总结一下运用神经网络算法的步骤：

     - Randomly initialize weights  (注意一定不能全部初始化为0)

     - Implement forward propagation to get $h_\Theta(x^{(i)})$ for any $x^{(i)}$

     - Implement code to compute cost function $J(\Theta)$

     - Implement backprop to compute partial derivatives $\frac{\partial}{\partial \Theta_{jk}^{(l)}}J(\Theta)$

     - Use gradient checking to compare $\frac{\partial}{\partial \Theta_{jk}^{(l)}}J(\Theta)$ computed using backpropagation vs. using numerical estimate of gradient  of $J(\Theta)$.

       Then disable gradient checking code

     - Use gradient descent or advanced optimization method with backpropagation to try to minimize $J(\Theta)$ as a function of parameters $\Theta$ 

9. 应用机器学习的建议

   - 按照6:2:2的比例将所有数据分为训练集、交叉验证集和测试集，用训练集来训练模型，用交叉验证集来选择交叉验证误差最小的模型，用测试集来估计模型的泛化误差。

   - **高偏差(high bias)－>欠拟合(underfitting)**：训练误差和交叉验证误差都很大；

     **高方差(high variance)－>过拟合(overfitting)**：训练误差小，交叉验证误差大。

   - 正则化项参数$\lambda$小的时候，容易过拟合；正则化项参数$\lambda$大的时候，容易欠拟合。

   - **学习曲线(learning curves)**：训练样本数量为自变量，误差为因变量。训练误差随着训练样本的增大而增大；交叉验证误差随着训练样本的增大而减小【正常情况】。

     高偏差(欠拟合)情况：交叉验证误差随着训练样本的增加先减小后不变；训练误差随着训练样本的增加先增加后不变，最后与交叉验证误差非常接近【原因：参数太少，拟合情况不好，导致误差很大】。因此在欠拟合情况下增加训练样本对改进算法没有太大帮助。

     高方差(过拟合)情况：训练误差随着训练样本的增加会增加一点，但不会变得很大；交叉验证误差会随着训练样本的增加而减少一点，但还是会非常大【特点：训练误差与交叉验证误差之间有很大的距离，交叉验证误差还有很大的下降空间】。因此在过拟合情况下增加训练样本有助于改进算法。

   - **改进学习算法**

     高方差问题：获得更多的训练样本；减少特征数量；增大正则化项参数$\lambda$。

     高偏差问题：选用更多的特征；增加多项式特征；减小正则化项参数$\lambda$。

   - **偏斜类(skewed classes)**：正负样本数量差别很大。如果出现了偏斜类这种情况，就不能简单的用误差来评估模型的好坏了，所以引入了查准率和查全率。

     **查准率(Precision)**：真正例(TP) / (真正例(TP) + 假正例(FP))；

     **召回率(Recall)**：真正例(TP) / (真正例(TP) + 假反例(FN))。

     $F_1 Score : 2\frac{PR}{P + R}$, 利用$F_1$来度量模型的好坏。

10. 
