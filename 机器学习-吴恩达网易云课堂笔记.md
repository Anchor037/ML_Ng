### 机器学习-吴恩达网易云课堂笔记

写在前面：这个视频已经全部看完了，可是看的时候有点赶进度，没有做笔记，导致学的并不是特别扎实。使自己想做笔记的导火索是在南科大参加夏令营面试的时候，老师问我怎么通过核函数将二维数据升维到三维，但是我当时只记得吴恩达视频里面讲过，但忘了具体怎么操作了，所以就没回答上来，这使我想要再看一遍视频并详细的做个笔记。既然学了，那就踏踏实实的学好！

1. Machine Learning: A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measure by P, improves with experience E.

2. Supervised learning, Unsupervised learning; Regression, Classification.

3. Linear regression
   - Hypothesis function : $h_\theta(x) = \theta_0 + \theta_1x$
   - Cost function : $J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2$   (Squared error function)
   - Goal : $\underset{\theta_0, \theta_1}{minimize} J(\theta_0, \theta_1)$

4.  Gradient descent

   - 用于最小化代价函数。这里以上述线性回归为例子描述该方法具体流程。

   - 算法过程 
     $$
     repeat\ until\ convergence \  \{            \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \ \\ 
         \theta_j := \theta_j - \alpha\frac{\partial }{\partial \theta_j}J(\theta_0, \theta_1) \quad (for \ j = 0 \ and \  j = 1)            \quad \quad \quad 　 \\
     \}   \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad   \quad\quad   \quad \quad \quad \quad \quad \quad \quad \quad \quad
     $$
     其中 $\alpha$称为学习率，$\alpha$越大代表梯度下降的越快，$\alpha$越小代表梯度下降的越慢。

     如果 $\alpha$太小的话梯度下降的会比较慢， $\alpha$太大的话梯度下降可能会越过最低点甚至无法收敛或者发散。

     注：$\theta_0$和$\theta_1$需要同时更新，更新方式如下：
     $$
     temp0 := \theta_0 - \alpha\frac{\partial }{\partial \theta_0}J(\theta_0, \theta_1)\\
     temp1 := \theta_1 - \alpha\frac{\partial }{\partial \theta_1}J(\theta_0, \theta_1)\\
     \theta_0 := temp0\\
     \theta_1 := temp1
     $$

   - Gradient descent for linear regression
     $$
     j = 0 :\ \frac{\partial }{\partial \theta_0}J(\theta_0, \theta_1) = \frac{1}{m}\sum_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)}) \quad\quad \\\
     j = 1 :\ \frac{\partial }{\partial \theta_1}J(\theta_0, \theta_1) = \frac{1}{m}\sum_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)})\cdot x^{(i)} 
     $$

   - 这种梯度下降算法也称为“Batch” Gradient descent, "Batch" : Each step of gradient descent uses all the training examples.

5. 
