### 机器学习-吴恩达网易云课堂笔记

1. **Machine Learning**: A computer program is said to learn from experience E with respect to some task T and some performance measure P, if its performance on T, as measure by P, improves with experience E.

2. **Supervised learning, Unsupervised learning; Regression, Classification.**

3. **Linear regression**
   - Hypothesis function : $h_\theta(x) = \theta_0 + \theta_1x$
   - Cost function : $J(\theta_0,\theta_1) = \frac{1}{2m}\sum_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2$   (Squared error function)
   - Goal : $\underset{\theta_0, \theta_1}{minimize} J(\theta_0, \theta_1)$

4. **Gradient descent**

   - 用于最小化代价函数。这里以上述线性回归为例子描述该方法具体流程。

   - 算法过程 
     $$
     repeat\ until\ convergence \  \{            \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \ \\ 
         \theta_j := \theta_j - \alpha\frac{\partial }{\partial \theta_j}J(\theta_0, \theta_1) \quad (for \ j = 0 \ and \  j = 1)            \quad \quad \quad 　 \\
     \}   \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad   \quad\quad   \quad \quad \quad \quad \quad \quad \quad \quad \quad
     $$
     其中 $\alpha$称为学习率，$\alpha$越大代表梯度下降的越快，$\alpha$越小代表梯度下降的越慢。

     如果 $\alpha$太小的话梯度下降的会比较慢， $\alpha$太大的话梯度下降可能会越过最低点甚至无法收敛或者发散。

     注：$\theta_0$和$\theta_1$需要同时更新，更新方式如下：
     $$
     temp0 := \theta_0 - \alpha\frac{\partial }{\partial \theta_0}J(\theta_0, \theta_1)\\
     temp1 := \theta_1 - \alpha\frac{\partial }{\partial \theta_1}J(\theta_0, \theta_1)\\
     \theta_0 := temp0\\
     \theta_1 := temp1
     $$

   - Gradient descent for linear regression
     $$
     j = 0 :\ \frac{\partial }{\partial \theta_0}J(\theta_0, \theta_1) = \frac{1}{m}\sum_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)}) \quad\quad \\\
     j = 1 :\ \frac{\partial }{\partial \theta_1}J(\theta_0, \theta_1) = \frac{1}{m}\sum_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)}) x^{(i)} \ \ \
     $$

   - 这种梯度下降算法也称为**“Batch” Gradient descent**, "Batch" : Each step of gradient descent uses all the training examples.

   - 梯度下降法的**缺点**：需要选择$\alpha$；需要多次迭代。

5. **Linear regression with multiple variables**

   - Hypothesis function : $h_\theta(x) = \theta^Tx = \theta_0x_0 + \theta_1x_1 + \theta_2x_2 + \cdots + \theta_nx_n$

   - Cost function : $J(\theta_0,\theta_1,\cdots ,\theta_n) = \frac{1}{2m}\sum_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)})^2$ 

   - Goal : $\underset{\theta_0, \theta_1, \cdots ,\theta_n}{minimize} J(\theta_0, \theta_1, \cdots ,\theta_n)$

   - Gradient descent
     $$
     repeat\  \{         \quad    \quad  \quad  \quad  \quad  \quad  \quad  \quad  \quad  \quad  \quad    \quad \quad \quad \quad \quad \quad \quad \quad  \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad  \ \\ 
         \theta_j := \theta_j - \alpha\frac{\partial }{\partial \theta_j}J(\theta_0, \theta_1, \dots, \theta_n) \quad (simultaneously \ update \ for \ every \ j = 0, \dots, n)               　 \\
     \}   \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad   \quad\quad   \quad \quad \quad \quad \quad \quad \quad \quad \quad  \quad  \quad  \quad  \quad  \quad  \quad  \quad  \quad  \quad    \quad \\
     其中\frac{\partial }{\partial \theta_ｊ}J(\theta_0, \theta_1, \dots, \theta_n) = \frac{1}{m}\sum_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)}) x_j^{(i)} \quad \quad \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad
     $$

   - **特征缩放(feature scaling)**，当不同的特征取值范围相差很大时，可以进行特征缩放，使所有的特征取值范围大致相同，这样可以加快梯度下降的速度。常用**均值归一化(mean normalization)**来进行特征缩放。

   - **正规方程法(normal equation)**，求解可得$\theta = (X^TX)^{-1}X^Ty$。注：由于需要计算$(X^TX)^{-1}$，故**当特征数很多的时候运行的非常慢**。

6. **Logistic regression**

   - 逻辑回归是一种**分类**算法。

   - Hypothesis  : $h_\theta(x) = g(\theta^Tx), \quad g(z) = \frac{1}{1 + e^{-z}}$.

   - predict "$y = 1$" if $h_\theta(x) \geqslant 0.5$ ; predict "$y = 0$" if $h_\theta(x) < 0.5$.

   - Cost function
     $$
     J(\theta) = \frac{1}{m}\sum_{i = 1}^{m}Cost(h_\theta(x^{(i)}), y^{(i)}) \\
     Cost(h_\theta(x), y) = \left\{
     \begin{aligned}
     -log(h_\theta(x)) \quad if \ y = 1\\
     -log(1 - h_\theta(x)) \quad if \ y = 0\\
     \end{aligned}
     \right.
     $$
     注：这里代价函数不用与线性回归一样的平方误差函数是因为$g(z) = \frac{1}{1 + e^{-z}}$是非线性函数，如果还用平方误差函数作为代价函数的话，会导致代价函数非凸，这样再用梯度下降法求解的话很容易陷入局部最优解。

     化简上述Cost等式，可得$Cost(h_\theta(x), y) = -ylog(h_\theta(x)) - (1 - y)log(1 - h_\theta(x))$，那么$J(\theta) = -\frac{1}{m}[\sum_{i = 1}^{m}y^{(i)}log(h_\theta(x^{(i)})) + (1 - y^{(i)})log(1 - h_\theta(x^{(i)}))]$

   - Gradient descent

     want $min_\theta J(\theta)$ :
     $$
     repeat\  \{      \quad  \quad  \quad  \quad  \quad  \quad  \quad  \quad  \quad    \quad \quad \quad \quad \quad \quad \quad \quad  \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad  \ \\ 
         \theta_j := \theta_j - \alpha\frac{\partial }{\partial \theta_j}J(\theta) \quad (simultaneously \ update \ all \ \theta_j)         \quad\quad\quad\quad\quad\quad\quad\quad      　 \\
     \}   \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad \quad   \quad\quad   \quad \quad \quad \quad \quad \quad \quad \quad \quad  \quad  \quad  \quad  \quad  \quad  \quad  \quad  \quad  \quad    \quad \\
     其中\frac{\partial }{\partial \theta_ｊ}J(\theta) = \frac{1}{m}\sum_{i = 1}^{m}(h_\theta(x^{(i)}) - y^{(i)})x_j^{(i)} \quad \quad \quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad\quad
     $$

7. **Regularization**

   - 用于减少**过拟合(overfitting)**问题。
   - 解决过拟合方法：减少特征数量；正则化。
   - Regularized linear regression : $J(\theta) = \frac{1}{2m}[\sum_{i = 1}^m(h_\theta(x^{(i)}) - y^{(i)})^2 + \lambda \sum_{j = 1}^n\theta_j^2]$
   - Regularized logistic regression : $J(\theta) = -\frac{1}{m}[\sum_{i = 1}^{m}y^{(i)}log(h_\theta(x^{(i)})) + (1 - y^{(i)})log(1 - h_\theta(x^{(i)}))] + \frac{\lambda}{2m}\sum_{j = 1}^n\theta_j^2$

8. **Neural Network**

   - 神经元：$h_\theta(x) = g(\theta^Tx), \quad g(z) = \frac{1}{1 + e^{-z}}$

   - 神经网络的基本模型

     <img src="/tmp/1563349400556.png" style="width:90px height:140px" /><img src="/tmp/1563349487592.png" style="width:90px height:135px" /> 
     $$
     a^{(2)}_1 = g(\Theta^{(1)}_{10}x_0 + \Theta^{(1)}_{11}x_1 + \Theta^{(1)}_{12}x_2 + \Theta^{(1)}_{13}x_3) \\ a^{(2)}_2 = g(\Theta^{(1)}_{20}x_0 + \Theta^{(1)}_{21}x_1 + \Theta^{(1)}_{22}x_2 + \Theta^{(1)}_{23}x_3) \\
     a^{(2)}_3 = g(\Theta^{(1)}_{30}x_0 + \Theta^{(1)}_{31}x_1 + \Theta^{(1)}_{32}x_2 + \Theta^{(1)}_{33}x_3) \\
     h_\Theta(x) = a^{(3)}_1 = g(\Theta^{(2)}_{10}a^{(2)}_0 +\Theta^{(2)}_{11}a^{(2)}_1 + \Theta^{(2)}_{12}a^{(2)}_2 + \Theta^{(2)}_{13}a^{(2)}_3)
     $$

   - 
